---
title: Open Calendars
---
My new favorite Socrata visualization is the calendar.

## Socrata visualization types
Socrata is more than a place to dump raw tables, or at least
it tries to be; you can make various charts and maps, and you
can serve non-tabular information to some degree.

There are many ways that a particular dataset could be visualized.
Socrata has `length(unique(socrata$displayType))` ways.
Here they are.

```{r display_types}
socrata$displayType <- factor(socrata$displayType, levels = names(sort(table(socrata$displayType), decreasing = T)))
ggplot(socrata) + aes(x = displayType) + geom_bar() + coord_flip()
```

(A lot of them are empty, and I don't know what's up with that.)

## People actually use calendars
The calendar is
[what it sounds like](https://data.oregon.gov/dataset/Public-Meetings-3-Month-View-/4775-yg3b?).
And people actually use them.

```{r calendar_use}
ggplot(subset(socrata.distinct, displayType == 'calendar')) + aes(x = viewCount) + geom_histogram() +
  scale_x_continuous('Hits per calendar', labels = comma) +
  scale_y_continuous('Frequency', labels = comma) +
  ggtitle("Someone likes her calendar.")
```

Let's look at some specific calendars.

```{r calendar_use_2}
calendar.use <- subset(socrata, displayType == 'calendar')[c('portal', 'id', 'viewCount')]
calendar.use[order(calendar.use$viewCount, decreasing = T),][1:5,c('portal', 'id', 'viewCount')]
```

The most viewed is Missouri's [open meetings calendar](https://data.mo.gov/Government-Administration/Open-Meetings-Calendar/mahp-izvx?).
They also have a different view of the calendar [outside the portal](http://www.mo.gov/meetings/),
with an RSS feed from the Socrata portal. But I don't know how the calendar on Socrata got so many views.

It looks like some other portals are using calendars a lot too,
but usually with several separate calendars instead of one huge one.

```{r calendar_use_3}
ggplot(sqldf('select portal, sum(viewCount) AS "total.view.count", count(*) AS "number.of.calendars" FROM "calendar.use" GROUP BY portal;')) +
  aes(x = number.of.calendars, y = total.view.count, label = portal) + geom_text() +
  scale_x_log10('Number of calendars in the portal', breaks = c(1,5,25)) +
  scale_y_log10('Total hits across all calendars', labels = comma, breaks = 10^(2:6))
```

(Like [Oregon](https://data.oregon.gov/browse?limitTo=calendars))

## The cool thing about Socrata calendars
The cool thing about Socrata calendars is that you can download them
as a spreadsheet.

There are lots of different calendar programs. Most of them are way
better for calendaring than Socrata. They also typically have import
and export tools for transferring your calendar between different
calendar tools.

But as far as I can tell, none of the main calendar programs lets
you export to a format that isn't special to. As far as I can tell,
Outlook, iCal and Google Calendar can import and export iCal files (`.ics`).
This lets you move your data among calendar programs, but it's
harder to connect them to non-calendar datasets.

<!--
http://www.zimbra.com/desktop/help/en_US/Calendar/Exporting_your_iCal_calendar.htm
http://office.microsoft.com/en-us/outlook-help/transfer-calendars-between-outlook-and-google-calendar-HA010167495.aspx
-->

### Analyzing Missouri's calendar
With Socrata, I can download Missouri's calendar as CSV and load it into R,


meetings <- read.csv('http://data.mo.gov/api/views/au6r-w9n3/rows.csv?accessType=DOWNLOAD', stringsAsFactors = F)
meetings$Begin.Date.Time <- strptime(meetings$Begin.Date.Time, format = '%m/%d/%Y %I:%M:00 %p')



### Finding other visualizations of the same data
You can even find examples of this within Socrata.
Look up all of Oregon's calendars,

```r
oregon.calendars <- subset(socrata, portal == 'data.oregon.gov' & displayType == 'calendar')
```

find out what datasets they visualize,

```r
barplot(sort(table(oregon.calendars$tableId), decreasing = T),
  horiz = T, las = 1, border = NA, col = 1,
  main = 'Oregon data tables that are visualized by calendars', ylab = '',
  xlab = 'Number of calendars visualizing the table'
)
```
look up one of the tables,

```r
table.429573 <- subset(socrata, tableId == 429573)[c('id', 'name', 'displayType')]
listify(table.429573)
```

and find that the calendar was just one of many visualizations of the same dataset!

### Combining calendars
I can also combine Oregon's and Missouri's public meetings calendars.

```r
oregon <- read.csv('http://data.oregon.gov/api/views/gs36-7t8m/rows.csv?accessType=DOWNLOAD', stringsAsFactors = F)
missouri <- read.csv('http://data.mo.gov/api/views/au6r-w9n3/rows.csv?accessType=DOWNLOAD', stringsAsFactors = F)

oregon.standard   <-   oregon[c('Agency..Board..Commission..Branch', 'Meeting.Title', 'Start.Date...Time', 'End.Date...Time', 'Meeting.Location', 'City', 'State')]
oregon.standard$State[oregon.standard$State == ''] <- 'OR'

missouri.standard <- missouri[c('Meeting.Body.Name', 'Committee', 'Begin.Date.Time', 'End.Date.Time', 'Address', 'City', 'State')]
missouri.standard$State <- 'MO'

names(oregon.standard) <- names(missouri.standard) <- c('Group', 'Meeting', 'Start', 'End', 'Street', 'City', 'State')
public.meetings <- rbind(oregon.standard, missouri.standard)
public.meetings$State <- factor(public.meetings$State, levels = c('OR', 'WA', 'MO'))
```

### Who has more meetings?
Oregon has more meetings.

```{r 
ggplot(public.meetings) + aes(x = State) + geom_bar()
```

### Meeting duration
Let's see how long the meetings are.

```{r meeting-length}
public.meetings$Start <- strptime(public.meetings$Start, format = '%m/%d/%Y %I:%M:00 %p')
public.meetings$End <- strptime(public.meetings$End, format = '%m/%d/%Y %I:%M:00 %p')
public.meetings$Duration <- as.numeric(public.meetings$End - public.meetings$Start) / 3600 # hours
ggplot(public.meetings) + aes(x = Start, y = Duration, color = State) + geom_jitter(alpha = 0.2)
```

A bunch of the meetings have end times before their start times. We could take a look at them like so.

    subset(public.meetings, Duration < 0)

But rather than figuring out what's wrong, let's just ignore them.

```{r meeting-length}
ggplot(subset(public.meetings, Duration > 0)) + aes(x = Start, y = Duration, color = State) +
  geom_jitter(alpha = 0.2)  + scale_y_log10('Duration (hours)', breaks = 10^(0:3))
```

Some of these meetings are pretty long. The three longest are each a month long.

```{r longest}
subset(public.meetings, Duration > 400)[c('Group', 'Meeting', 'Start', 'End')]
```

Maybe there are clusters of durations. Like maybe they're either an hour or two,
a day, a week or a month. I didn't look very hard, but seven clusters seems okay.

```{r clusters}
public.meetings.clean <- subset(public.meetings, !is.na(Duration) & Duration > 0)
clusterings <- list()
for (n in 1:10) {
  clustering <- kmeans(log10(public.meetings.clean$Duration), n)
  clusterings[[n]] <- clustering
  public.meetings.clean[paste0('cluster',n)] <- factor(clustering$cluster)
}

ggplot(public.meetings.clean) + aes(color = cluster7, x = 1, y = Duration) + geom_jitter(alpha = 0.2) +
  scale_y_log10() + scale_x_continuous("", breaks = c()) 
```

So the meeting durations seem clustered around these durations.
(The paranthetical duration is the mean duration for the cluster.)
<!-- sort(10^clusterings[[7]]$centers) -->

1. An hour (1.03 hours)
2. Two and a half hours (2.41 hours)
3. A workday (5.89 hours)
4. Two workdays (32.44 hours)
5. A work week (119.92 hours)
6. Two weeks (322.59 hours)
7. A month (714.32 hours)



### Day of week
What days of the week do they start usually on?

```{r day-of-week}
public.meetings$Day <- factor(
  strftime(public.meetings$Start, '%A'),
  levels = c('Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday'))
ggplot(public.meetings) + aes(x = Day, fill = State) + geom_bar()
```

holidays <- as.Date(c(
	'2005-01-17', '2005-02-21', '2005-05-30', '2005-07-04', '2005-09-05', '2005-10-10', '2005-11-11', '2005-11-24', '2005-12-26',
	'2006-01-02', '2006-01-16', '2006-02-20', '2006-05-29', '2006-07-04', '2006-09-04', '2006-10-09', '2006-11-10', '2006-11-23', '2006-12-25',
	'2007-01-01', '2007-01-15', '2007-02-19', '2007-05-28', '2007-07-04', '2007-09-03', '2007-10-08', '2007-11-12', '2007-11-22', '2007-12-25',
	'2008-01-01', '2008-01-21', '2008-02-18', '2008-05-26', '2008-07-04', '2008-09-01', '2008-10-13', '2008-11-11', '2008-11-27', '2008-12-25',
	'2009-01-01', '2009-01-19', '2009-02-16', '2009-05-25', '2009-07-03', '2009-09-07', '2009-10-12', '2009-11-11', '2009-11-26', '2009-12-25',
	'2010-01-01', '2010-01-18', '2010-02-15', '2010-05-31', '2010-07-05', '2010-09-06', '2010-10-11', '2010-11-11', '2010-11-25', '2010-12-24',
	'2010-12-31', '2011-01-17', '2011-02-21', '2011-05-30', '2011-07-04', '2011-09-05', '2011-10-10', '2011-11-11', '2011-11-24', '2011-12-26',
	'2012-01-02', '2012-01-16', '2012-02-20', '2012-05-28', '2012-07-04', '2012-09-03', '2012-10-08', '2012-11-12', '2012-11-22', '2012-12-25',
	'2013-01-01', '2013-01-21', '2013-02-18', '2013-05-27', '2013-07-04', '2013-10-14', '2013-11-11', '2013-11-28', '2013-12-25'))
```

## Thoughts

### Prevent data from becoming closed
Typical calendar software can import and export only from other calendar
software. If Socrata's calendar software were actually good, it would be
an example of calendar software that integrate with standard data analysis
software.

World Bank Open Finances seems to be trying this. They made a
[form](https://finances.worldbank.org/dataset/Global-Open-Data-Calendar-Entry-Form/qdbh-rfd3?)
that populates an
[open data events calendar](https://finances.worldbank.org/dataset/Global-Open-Data-Calendar/g4sx-dwxc).
This way, any data that is sent into the calendar immediately made available
to the public in various formats that can be used a wide variety of programs.

There's lots of siloed data in government, and we need better software and
methods for opening that up. But let's also make tools that prevent data from
becoming siloed in the first place.

We are producing much rich information when we do ordinary word processing,
scheduling, emailing, &c. The written documents themselves tell us quite a
bit, but so do simple things like the dates at which we were editing the
documents, the people we sent them to and just the number of documents we have.
You might not have realized it about it, but all of our software could export
this sort of information to standard tabular formats (for some concept of tabular).
If we want to embrace open data fully, we should be using programs that make data
open at their sources.

### Anything could be data, and data could be anything
For someone like me, it's not a big deal if standard calendar software
does not allow CSV export; I could easily have done the same analysis I did
above from iCal files, though it would have taken a bit longer. My larger
concern is that people don't think of calendars and other "apps" as data.

To me, anything could be turned into data, and data could be turned into
anything. For example, treasury cash flows that start
out as [nonstandard text files]() can be
turned into [tabular data](http://treasury.io) and then
[music](http://fms.csvsoundsystem.com).
Turns of a turnstile can recorded, stored in a
[really strange format](http://www.mta.info/developers/turnstile.html),
[parsed into a nicer format]()
and turned into [music](/!/ridership-rachenitsa).
And we could collect some information about a bunch of parking lots,
put it in a [data table]() and turn that into
[food]().

Much of our statistical
knowledge is based around a concept of a table, with columns as
variables (like "eye color") and rows as observations. (So each row might
be a different person.) This conceptual tabular representation is what
I think of as "data".

If we can represent the world as data, we can apply many quantitative
analytical methods to the data. First, we can convert data into other
data by combining datasets, building models, &c. And then we can convert
data back into real-world representations, like charts, apps, music and food.

But a lot of people don't realize this. I see this concept is a major part of
what I'll call "data literacy". That is, this is one fundamental thing
that [data scientists]() bizarrely understand. I propose that a lack of
understanding of this concept contributes to the siloing of data; teaching
this concept is an important part of the advance of open data.
